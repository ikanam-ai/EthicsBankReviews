{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "84efb11e-22f7-4faf-baf3-6c7742f6ef07",
   "metadata": {},
   "source": [
    "## Importing the libraries needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "01afcfa8-e08f-4a85-b03a-adc53e2d9e70",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-09 13:16:37.540143: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-04-09 13:16:37.643457: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-04-09 13:16:38.205827: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-04-09 13:16:38.205870: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-04-09 13:16:38.205874: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import stopwords\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.preprocessing import label_binarize\n",
    "import torch\n",
    "import seaborn as sns\n",
    "import transformers\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import RobertaModel, RobertaTokenizer\n",
    "import logging\n",
    "logging.basicConfig(level=logging.ERROR)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "748aebeb-e756-487c-9658-0cebb22e5fe2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch import cuda\n",
    "device = 'cuda' if cuda.is_available() else 'cpu'\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "46ad655d-d570-4c03-bc7e-3b100e8de13d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d175996e-2dc7-448c-80ad-3a9371226f9b",
   "metadata": {
    "tags": []
   },
   "source": [
    "## preproc and EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c4ca671a-de34-4797-a822-5f9242b4c5d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('../data/raw/train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "43f995a0-beda-49c1-a900-c024306c85e3",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "lGcvxwWXIbfq",
    "outputId": "e5fe3e91-acba-4249-9ce9-d18fb30bc575"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['+', '−', '?'], dtype=object)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['sentiment'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "18a7d8ce-494a-4faf-920b-87e61710d614",
   "metadata": {
    "_uuid": "4cc9d80f5b9969346c8f5ff24e3ce8de25dfc93d",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 297
    },
    "id": "y43HcyWgEadG",
    "outputId": "38ac5b75-2b34-4d13-8cbf-c56739ac6d02"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>19361.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>10755.525283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>6217.076236</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>5368.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>10757.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>16152.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>21512.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Unnamed: 0\n",
       "count  19361.000000\n",
       "mean   10755.525283\n",
       "std     6217.076236\n",
       "min        0.000000\n",
       "25%     5368.000000\n",
       "50%    10757.000000\n",
       "75%    16152.000000\n",
       "max    21512.000000"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f8fd3b8f-2574-4885-8409-d4a5e697a516",
   "metadata": {},
   "outputs": [],
   "source": [
    "sw = stopwords.words('russian')\n",
    "\n",
    "def clean_text(text):\n",
    "    \n",
    "    text = text.lower()\n",
    "    \n",
    "    text = re.sub(r\"[^а-яА-Я?.!,¿]+\", \" \", text) # replacing everything with space except (a-z, A-Z, \".\", \"?\", \"!\", \",\")\n",
    "\n",
    "    text = re.sub(r\"http\\S+\", \"\",text) #Removing URLs \n",
    "    #text = re.sub(r\"http\", \"\",text)\n",
    "    \n",
    "    html=re.compile(r'<.*?>') \n",
    "    \n",
    "    text = html.sub(r'',text) #Removing html tags\n",
    "    \n",
    "    punctuations = '@#!?+&*[]-%.:/();$=><|{}^' + \"'`\" + '_'\n",
    "    for p in punctuations:\n",
    "        text = text.replace(p,'') #Removing punctuations\n",
    "        \n",
    "    text = [word.lower() for word in text.split() if word.lower() not in sw]\n",
    "    \n",
    "    text = \" \".join(text) #removing stopwords\n",
    "    \n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                           u\"\\U00002702-\\U000027B0\"\n",
    "                           u\"\\U000024C2-\\U0001F251\"\n",
    "                           \"]+\", flags=re.UNICODE)\n",
    "    text = emoji_pattern.sub(r'', text) #Removing emojis\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d4b5c77a-2e24-41e8-aa62-0de7df4b7db0",
   "metadata": {
    "_uuid": "01da38cc4626a85b73fbb526d9a8d128d1fd9338",
    "id": "baSmeDdIEadM"
   },
   "outputs": [],
   "source": [
    "new_df = train[['sentence', 'sentiment']]\n",
    "new_df['sentence'] = new_df['sentence'].apply(lambda x: clean_text(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d1b3f896-8a5c-4a1b-b8cf-9fadc44520df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>получал качественные услуги</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>вижу, хотя поставить, сервис</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>любимый банк мкб обманул</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>отвратительное отношение клиентам</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>любое время дня ночи помогут, ответят, решат</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19356</th>\n",
       "      <td>коем случае открывайте счет недостойном довери...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19357</th>\n",
       "      <td>ти откровенно забили качество развивают свои м...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19358</th>\n",
       "      <td>считаю, это прорыв лидерство финансовых услуг ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19359</th>\n",
       "      <td>писал мужчина очень доходчиво, финансовым язык...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19360</th>\n",
       "      <td>данная ситуация сильно выбила колеи, и, вместо...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>19361 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                sentence  sentiment\n",
       "0                            получал качественные услуги          2\n",
       "1                           вижу, хотя поставить, сервис          0\n",
       "2                               любимый банк мкб обманул          0\n",
       "3                      отвратительное отношение клиентам          0\n",
       "4           любое время дня ночи помогут, ответят, решат          2\n",
       "...                                                  ...        ...\n",
       "19356  коем случае открывайте счет недостойном довери...          0\n",
       "19357  ти откровенно забили качество развивают свои м...          0\n",
       "19358  считаю, это прорыв лидерство финансовых услуг ...          2\n",
       "19359  писал мужчина очень доходчиво, финансовым язык...          2\n",
       "19360  данная ситуация сильно выбила колеи, и, вместо...          0\n",
       "\n",
       "[19361 rows x 2 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict_lavel = {'+':2, '?':1, '−':0}\n",
    "new_df['sentiment'] = new_df['sentiment'].apply(lambda x: dict_lavel[x])\n",
    "new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5b15abe3-8c01-4854-91d5-8ef98df6c775",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 115,
     "referenced_widgets": [
      "30864762e7f242c281b72862c5c08a33",
      "dd12a39995584ba79f0e786b370b1a99",
      "b89c9e76b5594a8ea601b9c5d2af4fa6",
      "f65c7649640b4e87a7819a3da2f54fe0",
      "80a6b6c9c4d5436ebe3b90b791c6fd93",
      "8c1f6e94723842faa6bd3dcd9ff4ea82",
      "39b5ca071fd3452e9d9145dd2b366da1",
      "f8dfd3ea6bb7413592115195bc6e0b83",
      "611dfdca86f4498e8aa1491ed6ffb13d",
      "be4857f17c244fb39a771f2c97283fd5",
      "2fe41e1db18b4295a6907771462a0fce",
      "0b29a9e1a275451bbc2114807532f91e",
      "115c8809853d410fac6e7f69af5a5488",
      "390827d7d2cb4b4fbfc0c022f015f7ed",
      "5d305d4db08f47ba91461edb343874a4",
      "773af3ca0add4e7cac0a036fb8b55632"
     ]
    },
    "id": "nvXxpfNCGER2",
    "outputId": "d7281fe1-0dbf-42d7-c1e0-b51c4231c9c0"
   },
   "outputs": [],
   "source": [
    "# Defining some key variables that will be used later on in the training\n",
    "MAX_LEN = 256\n",
    "TRAIN_BATCH_SIZE = 24\n",
    "VALID_BATCH_SIZE = 48\n",
    "# EPOCHS = 1\n",
    "LEARNING_RATE = 1e-05\n",
    "tokenizer = RobertaTokenizer.from_pretrained('ai-forever/ruRoberta-large', truncation=True, do_lower_case=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fc13b71f-229e-47e1-8b83-dd05eba4a63b",
   "metadata": {
    "id": "3vWRDemOGxJD"
   },
   "outputs": [],
   "source": [
    "class SentimentData(Dataset):\n",
    "    def __init__(self, dataframe, tokenizer, max_len):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.data = dataframe\n",
    "        self.text = dataframe.sentence\n",
    "        self.targets = self.data.sentiment\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.text)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        text = str(self.text[index])\n",
    "        text = \" \".join(text.split())\n",
    "\n",
    "        inputs = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            None,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            pad_to_max_length=True,\n",
    "            return_token_type_ids=True\n",
    "        )\n",
    "        ids = inputs['input_ids']\n",
    "        mask = inputs['attention_mask']\n",
    "        token_type_ids = inputs[\"token_type_ids\"]\n",
    "\n",
    "\n",
    "        return {\n",
    "            'ids': torch.tensor(ids, dtype=torch.long),\n",
    "            'mask': torch.tensor(mask, dtype=torch.long),\n",
    "            'token_type_ids': torch.tensor(token_type_ids, dtype=torch.long),\n",
    "            'targets': torch.tensor(self.targets[index], dtype=torch.float)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e7b9a4a8-5455-4127-8935-73803571f76e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "id": "7Gpe9D1QHoCd",
    "outputId": "7fc7fc2e-68a2-44b7-8e80-6bb6ce6c178b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FULL Dataset: (19361, 2)\n",
      "TRAIN Dataset: (15489, 2)\n",
      "TEST Dataset: (3872, 2)\n"
     ]
    }
   ],
   "source": [
    "train_size = 0.8\n",
    "train_data=new_df.sample(frac=train_size,random_state=42)\n",
    "\n",
    "test_data=new_df.drop(train_data.index)\n",
    "test_data_index = test_data.index.tolist()\n",
    "test_data=test_data.reset_index(drop=True)\n",
    "train_data = train_data.reset_index(drop=True)\n",
    "\n",
    "\n",
    "print(\"FULL Dataset: {}\".format(new_df.shape))\n",
    "print(\"TRAIN Dataset: {}\".format(train_data.shape))\n",
    "print(\"TEST Dataset: {}\".format(test_data.shape))\n",
    "\n",
    "training_set = SentimentData(train_data, tokenizer, MAX_LEN)\n",
    "testing_set = SentimentData(test_data, tokenizer, MAX_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d4f6e414-0ea4-4b56-bd9f-d501bc12cb6b",
   "metadata": {
    "_uuid": "9fc198d13d7f33dc70588c3f22bc7b7c4f4ebb45",
    "id": "c1tInLk2Eadt"
   },
   "outputs": [],
   "source": [
    "train_params = {'batch_size': TRAIN_BATCH_SIZE,\n",
    "                'shuffle': True,\n",
    "                'num_workers': 0\n",
    "                }\n",
    "\n",
    "test_params = {'batch_size': VALID_BATCH_SIZE,\n",
    "                'shuffle': True,\n",
    "                'num_workers': 0\n",
    "                }\n",
    "\n",
    "training_loader = DataLoader(training_set, **train_params)\n",
    "testing_loader = DataLoader(testing_set, **test_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e4303c2-e501-4ca7-8b19-f7130d4ee0b6",
   "metadata": {},
   "source": [
    "## training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "93089359-18f4-4d73-b024-fa55e291b0a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ai-forever/ruRoberta-large were not used when initializing RobertaModel: ['lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.decoder.weight', 'lm_head.dense.bias', 'lm_head.decoder.bias', 'lm_head.dense.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at ai-forever/ruRoberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ok\n"
     ]
    }
   ],
   "source": [
    "class MTnluClass(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MTnluClass, self).__init__()\n",
    "        self.l1 = RobertaModel.from_pretrained(\"ai-forever/ruRoberta-large\")\n",
    "        self.pre_classifier = torch.nn.Linear(1024, 1024)\n",
    "        self.dropout = torch.nn.Dropout(0.3)\n",
    "        self.classifier = torch.nn.Linear(1024, 3)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, token_type_ids):\n",
    "        output_1 = self.l1(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n",
    "        hidden_state = output_1[0]\n",
    "        pooler = hidden_state[:, 0]\n",
    "        pooler = self.pre_classifier(pooler)\n",
    "        pooler = torch.nn.ReLU()(pooler)\n",
    "        pooler = self.dropout(pooler)\n",
    "        output = self.classifier(pooler)\n",
    "        return output\n",
    "    \n",
    "    def get_embed(self, input_ids, attention_mask, token_type_ids):\n",
    "        output_1 = self.l1(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n",
    "        hidden_state = output_1[0]\n",
    "        pooler = hidden_state[:, 0]\n",
    "        return pooler\n",
    "    \n",
    "model = MTnluClass()\n",
    "model.load_state_dict(torch.load('/app/hsehack_2023/models/rorubert87.pth', map_location='cpu'))\n",
    "model.to(device)\n",
    "print('ok')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f7ddf1d8-1f03-4cc5-9963-60480378647f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the loss function and optimizer\n",
    "loss_function = torch.nn.CrossEntropyLoss()\n",
    "LEARNING_RATE = 1e-05\n",
    "optimizer = torch.optim.Adam(params =  model.parameters(), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f501fe3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ai-forever/ruRoberta-large were not used when initializing RobertaModel: ['lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.decoder.weight', 'lm_head.dense.bias', 'lm_head.decoder.bias', 'lm_head.dense.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at ai-forever/ruRoberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ok\n"
     ]
    }
   ],
   "source": [
    "class MTnluClass(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MTnluClass, self).__init__()\n",
    "        self.l1 = RobertaModel.from_pretrained(\"ai-forever/ruRoberta-large\")\n",
    "        self.pre_classifier = torch.nn.Linear(1024, 1024)\n",
    "        self.dropout = torch.nn.Dropout(0.3)\n",
    "        self.classifier = torch.nn.Linear(1024, 3)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, token_type_ids):\n",
    "        output_1 = self.l1(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n",
    "        hidden_state = output_1[0]\n",
    "        pooler = hidden_state[:, 0]\n",
    "        pooler = self.pre_classifier(pooler)\n",
    "        pooler = torch.nn.ReLU()(pooler)\n",
    "        pooler = self.dropout(pooler)\n",
    "        output = self.classifier(pooler)\n",
    "        return output\n",
    "    \n",
    "    def get_embed(self, input_ids, attention_mask, token_type_ids):\n",
    "        output_1 = self.l1(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n",
    "        hidden_state = output_1[0]\n",
    "        pooler = hidden_state[:, 0]\n",
    "        return pooler\n",
    "    \n",
    "model = MTnluClass()\n",
    "model.load_state_dict(torch.load('/app/hsehack_2023/models/rorubert87.pth', map_location='cpu'))\n",
    "model.to(device)\n",
    "print('ok')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9cedf78e-a470-4bab-b4df-5a4d3ee45478",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_roc_auc(preds, targets):\n",
    "    # proba_dict = {0:np.array([1.,0.,0.]),\n",
    "    #               1:np.array([0.,1.,0.]),\n",
    "    #               2:np.array([0.,0.,1.])}\n",
    "    # targets = np.array(list(map(lambda x: proba_dict[x], targets.cpu().numpy())))\n",
    "    # preds = np.array(list(map(lambda x: proba_dict[x], preds.cpu().numpy())))\n",
    "    # print(preds)\n",
    "    # print(targets)\n",
    "    y_preds = label_binarize(preds.cpu().numpy(), classes=[0,1,2])\n",
    "    return roc_auc_score(y_preds, targets.cpu().numpy(), multi_class='ovr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e326586e-d590-4fa8-b0f7-b741c97e9fc0",
   "metadata": {
    "id": "mhqvtY2SIup7",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss per 5000 steps: 0.1246979609131813\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "646it [07:52,  1.37it/s]\n",
      "81it [00:44,  1.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss Epoch: 0.28332379932051105\n",
      "valid Roc Auc Epoch: 0.8806997916189184\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss per 5000 steps: 0.2563576400279999\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "116it [01:24,  1.37it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 17\u001b[0m\n\u001b[1;32m     15\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(ids, mask, token_type_ids)\n\u001b[1;32m     16\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_function(outputs, targets)\n\u001b[0;32m---> 17\u001b[0m tr_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m big_val, big_idx \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmax(outputs\u001b[38;5;241m.\u001b[39mdata, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     19\u001b[0m y_preds \u001b[38;5;241m=\u001b[39m label_binarize(big_idx\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy(), classes\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m2\u001b[39m])\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Defining the training function on the 80% of the dataset for tuning the distilbert model\n",
    "epochs = 5\n",
    "for epoch in range(0, epochs):\n",
    "    tr_loss = 0\n",
    "    n_correct = 0\n",
    "    nb_tr_steps = 0\n",
    "    nb_tr_examples = 0\n",
    "    model.train()\n",
    "    for _,data in tqdm(enumerate(training_loader, 0)):\n",
    "        ids = data['ids'].to(device, dtype = torch.long)\n",
    "        mask = data['mask'].to(device, dtype = torch.long)\n",
    "        token_type_ids = data['token_type_ids'].to(device, dtype = torch.long)\n",
    "        targets = data['targets'].to(device, dtype = torch.long)\n",
    "\n",
    "        outputs = model(ids, mask, token_type_ids)\n",
    "        loss = loss_function(outputs, targets)\n",
    "        tr_loss += loss.item()\n",
    "        big_val, big_idx = torch.max(outputs.data, dim=1)\n",
    "        y_preds = label_binarize(big_idx.cpu().numpy(), classes=[0,1,2])\n",
    "        nb_tr_steps += 1\n",
    "        nb_tr_examples+=targets.size(0)\n",
    "        \n",
    "        if _%5000==0:\n",
    "            loss_step = tr_loss/nb_tr_steps\n",
    "            print(f\"Training Loss per 5000 steps: {loss_step}\")\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        # # When using GPU\n",
    "        optimizer.step()\n",
    "        \n",
    "    y_preds_test = []\n",
    "    targets_test = []\n",
    "    for _,data in tqdm(enumerate(testing_loader, 0)):\n",
    "        ids = data['ids'].to(device, dtype = torch.long)\n",
    "        mask = data['mask'].to(device, dtype = torch.long)\n",
    "        token_type_ids = data['token_type_ids'].to(device, dtype = torch.long)\n",
    "        targets = data['targets'].to(device, dtype = torch.long)\n",
    "        with torch.no_grad():\n",
    "            outputs = model(ids, mask, token_type_ids)\n",
    "            big_val, big_idx = torch.max(outputs.data, dim=1)\n",
    "            y_preds = label_binarize(big_idx.cpu().numpy(), classes=[0,1,2])\n",
    "            y_preds_test += y_preds.tolist()\n",
    "            targets_test += targets.cpu().tolist()\n",
    "            \n",
    "    epoch_loss = tr_loss/nb_tr_steps     \n",
    "    print(f\"Training Loss Epoch: {epoch_loss}\") \n",
    "    print(f\"valid Roc Auc Epoch: {roc_auc_score(np.array(targets_test), np.array(y_preds_test), multi_class='ovr')}\")\n",
    "    #print(f'The Total Accuracy for Epoch {epoch}: {(n_correct*100)/nb_tr_examples}')\n",
    "    \n",
    "    # epoch_accu = (n_correct*100)/nb_tr_examples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "872fdf1a-9b2b-4a0d-9778-d6b108c75df8",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), '/app/hsehack_2023/models/rorubert88_n.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cf73f91-64d3-4050-88c0-a87ae6292be2",
   "metadata": {},
   "source": [
    "## get embed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d50a5ac2-8615-43e5-b316-0afb9170f450",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "488it [02:13,  3.63it/s]"
     ]
    }
   ],
   "source": [
    "outputs_train = []\n",
    "targets_train = []\n",
    "with torch.no_grad():\n",
    "    for _,data in tqdm(enumerate(training_loader, 0)):\n",
    "        ids = data['ids'].to(device, dtype = torch.long)\n",
    "        mask = data['mask'].to(device, dtype = torch.long)\n",
    "        token_type_ids = data['token_type_ids'].to(device, dtype=torch.long)\n",
    "        targets = data['targets'].to(device, dtype = torch.long)\n",
    "        embed = model.get_embed(ids, mask, token_type_ids)\n",
    "        outputs_train += embed.cpu().numpy().tolist()\n",
    "        targets_train += targets.cpu().numpy().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "7a62605d-e6f8-46d4-9e3c-05f317b60b61",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (8,) (24,1024) ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[34], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m outputs_train \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m embed\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[1;32m      2\u001b[0m outputs_train\n",
      "\u001b[0;31mValueError\u001b[0m: operands could not be broadcast together with shapes (8,) (24,1024) "
     ]
    }
   ],
   "source": [
    "outputs_train. embed.cpu().numpy()\n",
    "outputs_train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3e1bd1c-5db0-4202-97a1-ffdc02431c2e",
   "metadata": {
    "tags": []
   },
   "source": [
    "## inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f845b7d6-b047-4842-a06c-6f5bf547777d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ids_all = []\n",
    "mask_all = []\n",
    "token_type_ids_all = []\n",
    "for num in new_df.sentence.tolist():\n",
    "    inputs = tokenizer.encode_plus(\n",
    "                num,\n",
    "                None,\n",
    "                add_special_tokens=True,\n",
    "                max_length=256,\n",
    "                pad_to_max_length=True,\n",
    "                return_token_type_ids=True\n",
    "            )\n",
    "    ids = inputs['input_ids']\n",
    "    mask = inputs['attention_mask']\n",
    "    token_type_ids = inputs[\"token_type_ids\"]\n",
    "    \n",
    "    ids_all += [ids]\n",
    "    mask_all += [mask]\n",
    "    token_type_ids_all += [token_type_ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "71fa458a-0d64-4174-8d1f-63dad7fcf3c9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
